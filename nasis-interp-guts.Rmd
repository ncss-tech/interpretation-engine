---
title: "Extracting NASIS Rules/Evaluations"
author: "D.E. Beaudette and J. Nemecek"
date: "`r Sys.Date()`"
output:
  html_document:
    mathjax: null
    jquery: null
    smart: no
    keep_md: FALSE
---

  
```{r setup, echo=FALSE, results='hide', warning=FALSE}
library(knitr, quietly = TRUE)
opts_chunk$set(message=FALSE, warning=FALSE, fig.retina = 2, fig.align='center', dev='png', dev.args=list(pointsize=10, antialias='cleartype'), tidy=FALSE)
options(width=500, stringsAsFactors=FALSE)
```



```{r, fig.width=6, fig.height=4}
library(soilDB)
library(RODBC)
library(XML)
library(plyr)
library(data.tree)
library(digest)
library(jsonlite)
library(sharpshootR)

# source local functions
source('local-functions.R')

# re-load cached data
# getAndCacheData()

# load cached data
load('cached-NASIS-data.Rda')
```


## Evaluation Curves
Someimes the property min/max values are crazy, you can manually set plot limits with `xlim` argument.
```{r, fig.width=6, fig.height=4}
# arbitrary curve:
e <- evals[evals$evalname == '*Storie Factor C Slope 0 to 100%', ]
plotEvaluation(e, xlim=c(0, 200))

# get a function to describe the evaluation
s <- seq(0, 100, length.out = 10)
f <- extractEvalCurve(e)
cbind(domain=s, fuzzy.rating=f(s))

## sigmoid:
e <- evals[evals$evalname == 'SAR, WTD_AVG 0 to 100cm', ]
plotEvaluation(e, xlim=c(0,25))

# arbitrary linear
e <- evals[evals$evalname == 'GRL - EC maximum in depth 25 to 50 cm (NV)', ]
plotEvaluation(e, xlim=c(0, 1000))

# linear
e <- evals[evals$evalname == 'Slope 0 to >15%', ]
plotEvaluation(e)

# crisp:
e <- evals[evals$evalname == 'Soil pH (water) >= 4.5 and <= 8.4, 0-100cm', ]
plotEvaluation(e)

e <- evals[evals$evalname == 'Available Water Capacity <10cm', ]
plotEvaluation(e)
```

Evaluate an arbitrary rule, with an arbitrary value.
*NCCPI - pH 0-20cm (Corn and Soybeans)* and pH = 6.5
```{r}
e <- evals[evals$evalname == 'NCCPI - pH 0-20cm (Corn and Soybeans)', ]
extractEvalCurve(e)(6.5)
```


## Rule Trees
* check a couple, RefId points to rows in rules or evaluation tables
* the dt$Do call links child sub-rules
* recursion is used to traverse to the deepest nodes, seems to work
* **caution:** don't run several times on the same object!
* increase limit argument to see the entire tree

```{r, fig.width=6, fig.height=4}
# just some random rule
y <- rules[rules$rulename == 'Dust PM10 and PM2.5 Generation', ]

dt <- parseRule(y)

# print intermediate results
print(dt, 'Type', 'Value', 'RefId', 'rule_refid', 'eval_refid', limit=25)

# recusively splice-in sub-rules
dt$Do(traversal='pre-order', fun=linkSubRules)
# splice-in evaluation functions, if possible
dt$Do(traversal='pre-order', fun=linkEvaluationFunctions)

# print more attributes
options(width=300)
print(dt, 'Type', 'Value', 'RefId', 'rule_refid', 'eval_refid', 'evalType', limit=25)

print(dt, 'Type', 'Value', 'evalType', 'propname', 'propiid', 'propuom', limit=25)
```

Get a table of unique evaluations, properties, and property IDs
```{r, fig.width=6, fig.height=4}
(ps <- getPropertySet(dt))
```

### Properties via NASIS WebReport
Experimental and subject to change. Can specify multiple componet records but only a single property.
```{r}
url <- 'https://nasis.sc.egov.usda.gov/NasisReportsWebSite/limsreport.aspx?report_name=WEB-PROPERY-COMPONENT_property'

# prop_id: property ID (10244 = AWC, 0-50CM OR FIRST RESTRICTIVE LAYER)
# cokey: component record ID, can be a comma-delim list
args <- list(prop_id='10244', cokey='1842387')

res <- parseWebReport(url, args, index=1)
kable(res)
```

More useful, connect the property back to the name of the property.
```{r results='hide'}
props <- lookupProperties(unique(ps$propiid), coiid='1842387')
```

```{r}
# join with properties
z <- join(ps, props, by='propiid', type='left')

kable(z)
```



### More Examples

**WLF-Soil Suitability - Karner Blue Butterfly**
```{r, fig.width=6, fig.height=4}
y <- rules[rules$rulename == 'WLF-Soil Suitability - Karner Blue Butterfly', ]
dt <- parseRule(y)
dt$Do(traversal='pre-order', fun=linkSubRules)
dt$Do(traversal='pre-order', fun=linkEvaluationFunctions)
print(dt, 'Type', 'Value', 'RefId', 'rule_refid', 'eval_refid', 'evalType', limit=25)
```

```{r, fig.width=6, fig.height=4}
ps <- getPropertySet(dt)
kable(ps)
```


```{r, fig.width=6, fig.height=4}
e <- evals[evals$evalname == 'KSAT MINIMUM IN DEPTH TO 50 cm  or above restriction', ]
plotEvaluation(e, xlim=c(0, 200))

e <- evals[evals$evalname == 'KARNER-DEPTH TO HIGH WATER TABLE, Growing Season MIN', ]
plotEvaluation(e, xlim=c(0, 200))

e <- evals[evals$evalname == 'KARNER-AWS, 0-50CM OR FIRST RESTRICTIVE LAYER', ]
plotEvaluation(e, xlim=c(0, 200))

## dangit, haven't implemented this kind of evaluation yet
# e <- evals[evals$evalname == 'WICCPI - Interpretable Component', ]
# plotEvaluation(e, xlim=c(0, 200))
```


**California Revised Storie Index**
```{r, fig.width=6, fig.height=4}
y <- rules[rules$rulename == 'AGR - California Revised Storie Index (CA)', ]
dt <- parseRule(y)
dt$Do(traversal='pre-order', fun=linkSubRules)
dt$Do(traversal='pre-order', fun=linkEvaluationFunctions)
print(dt, 'Type', 'Value', 'RefId', 'rule_refid', 'eval_refid', 'evalType', limit=25)
```

```{r, fig.width=6, fig.height=4}
options(width=80)
ps <- getPropertySet(dt)
unique(ps$propname)
```


**NCCPI Version 3.0**
```{r, fig.width=6, fig.height=4, eval=FALSE}
y <- rules[rules$rulename == 'NCCPI - National Commodity Crop Productivity Index (Ver 3.0)', ]
dt <- parseRule(y)
dt$Do(traversal='pre-order', fun=linkSubRules)
dt$Do(traversal='pre-order', fun=linkEvaluationFunctions)

options(width=500, max.print = 1e6)
sink(file = 'examples/nccpi-full-tree.txt')
print(dt, 'Type', 'Value', 'RefId', 'rule_refid', 'eval_refid', 'evalType', 'propname', 'propiid', limit=NULL)
sink()

options(max.print=1000, width=80)

# save properties and IDs to CSV
ps <- getPropertySet(dt)
write.csv(ps, file='examples/nccpi-properties.csv', row.names = FALSE)

# flatten tree to data.frame and save to CSV
d <- ToDataFrameTable(dt, 'name', 'Type', 'Value', 'RefId', 'rule_refid', 'eval_refid', 'evalType', 'propname', 'propiid')
write.csv(unique(d), file='examples/nccpi-table-representation.csv', row.names = FALSE)
```


**Commodity Crop Productivity Index (Corn) (WI)**
```{r, eval=FALSE}
y <- rules[rules$rulename == 'Commodity Crop Productivity Index (Corn) (WI)', ]
dt <- parseRule(y)
dt$Do(traversal='pre-order', fun=linkSubRules)
dt$Do(traversal='pre-order', fun=linkEvaluationFunctions)

options(width=500, max.print = 1e6)
sink(file = 'examples/CCPI-WI-full-tree.txt')
print(dt, 'Type', 'Value', 'RefId', 'rule_refid', 'eval_refid', 'evalType', 'propname', 'propiid', limit=NULL)
sink()

options(max.print=1000, width=200)

ps <- getPropertySet(dt)
write.csv(unique(ps), file='examples/CCPI-WI-properties.csv', row.names = FALSE)

d <- ToDataFrameTable(dt, 'name', 'Type', 'Value', 'RefId', 'rule_refid', 'eval_refid', 'evalType', 'propname', 'propiid')
write.csv(unique(d), file='examples/CCPI-WI-table-representation.csv', row.names = FALSE)

```


Get all properties that this interpretation requires, for a given component. Runtime: 1 minute. Some requests result in an error, due to request time-out or SSL errors--why? Only happens some times.
```{r, eval=FALSE}
(props <- lookupProperties(unique(ps$propiid), coiid='1842387'))

# join with properties
z <- join(ps, props, by='propiid', type='left')
```


```{r, fig.width=6, fig.height=4}
y <- rules[rules$rulename == 'Commodity Crop Productivity Index (Corn) (WI)', ]
dt <- parseRule(y)
dt$Do(traversal='pre-order', fun=linkSubRules)
dt$Do(traversal='pre-order', fun=linkEvaluationFunctions)
print(dt, 'Type', 'Value', 'RefId', 'rule_refid', 'eval_refid', 'evalType', limit=25)



y <- rules[rules$rulename == 'DHS - Catastrophic Mortality, Large Animal Disposal, Pit', ]
dt <- parseRule(y)
dt$Do(traversal='pre-order', fun=linkSubRules)
dt$Do(traversal='pre-order', fun=linkEvaluationFunctions)
print(dt, 'Type', 'Value', 'RefId', 'rule_refid', 'eval_refid', 'evalType', limit=25)


# this one has a single RuleEvaluation: RefId points to rows in evals table
y <- rules[rules$rulename == 'Clay %, in surface - MO2', ]
dt <- parseRule(y)
dt$Do(traversal='pre-order', fun=linkSubRules)
dt$Do(traversal='pre-order', fun=linkEvaluationFunctions)
print(dt, 'Type', 'Value', 'RefId', 'rule_refid', 'eval_refid', 'evalType', limit=25)
```

## Conversion to JSON and D3 Vizualization
The objects can't have evaluation functions attached
```{r, eval=FALSE}

y <- rules[rules$rulename == 'Commodity Crop Productivity Index (Corn) (WI)', ]
dt <- parseRule(y)
dt$Do(traversal='pre-order', fun=linkSubRules)
dt$Do(traversal='pre-order', fun=linkEvaluationFunctions)

# prep for use in D3
# note arguments for D3 JSON compatibility
dt.list <- ToListExplicit(dt, unname = TRUE, nameName = "name", childrenName = "children")
dt.json <- toJSON(dt.list, pretty = TRUE, auto_unbox = TRUE, force=TRUE)

# save local copy of JSON
cat(dt.json, file='D3/dt.json')


# test with:
# "C:\Program Files (x86)\PYTHON27\ArcGIS10.1\python.exe" -m SimpleHTTPServer

```


